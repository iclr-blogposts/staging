@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{tirumala2022memorization,
  title={Memorization without overfitting: Analyzing the training dynamics of large language models},
  author={Tirumala, Kushal and Markosyan, Aram H and Zettlemoyer, Luke and Aghajanyan, Armen},
  journal={arXiv preprint arXiv:2205.10770},
  year={2022}
}
@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}
@article{chen2021bert2bert,
  title={bert2bert: Towards reusable pretrained language models},
  author={Chen, Cheng and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Qin, Yujia and Wang, Fengyu and Wang, Zhi and Chen, Xiao and Liu, Zhiyuan and Liu, Qun},
  journal={arXiv preprint arXiv:2110.07143},
  year={2021}
}
@article{bahdanau2017learning,
  title={Learning to compute word embeddings on the fly},
  author={Bahdanau, Dzmitry and Bosc, Tom and Jastrz{\k{e}}bski, Stanis{\l}aw and Grefenstette, Edward and Vincent, Pascal and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1706.00286},
  year={2017}
}
@article{gong2018frage,
  title={Frage: Frequency-agnostic word representation},
  author={Gong, Chengyue and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{khassanov2019constrained,
  title={Constrained output embeddings for end-to-end code-switching speech recognition with only monolingual data},
  author={Khassanov, Yerbolat and Xu, Haihua and Pham, Van Tung and Zeng, Zhiping and Chng, Eng Siong and Ni, Chongjia and Ma, Bin},
  journal={arXiv preprint arXiv:1904.03802},
  year={2019}
}
@article{schick2020s,
  title={It's not just size that matters: Small language models are also few-shot learners},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2009.07118},
  year={2020}
}
@inproceedings{wu2021taking,
  title={Taking notes on the fly helps language pre-training},
  author={Wu, Qiyu and Xing, Chen and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}
@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@inproceedings{gong2019efficient,
  title={Efficient training of bert by progressively stacking},
  author={Gong, Linyuan and He, Di and Li, Zhuohan and Qin, Tao and Wang, Liwei and Liu, Tieyan},
  booktitle={International conference on machine learning},
  pages={2337--2346},
  year={2019},
  organization={PMLR}
}

@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{gao2019representation,
  title={Representation degeneration problem in training natural language generation models},
  author={Gao, Jun and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1907.12009},
  year={2019}
}

@article{gong2018frage,
  title={Frage: Frequency-agnostic word representation},
  author={Gong, Chengyue and He, Di and Tan, Xu and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{makany2009optimising,
  title={Optimising the use of note-taking as an external cognitive aid for increasing learning},
  author={Makany, Tamas and Kemp, Jonathan and Dror, Itiel E},
  journal={British Journal of Educational Technology},
  volume={40},
  number={4},
  pages={619--635},
  year={2009},
  publisher={Wiley Online Library}
}
@article{feng2022memory,
  title={Memory Augmented Lookup Dictionary based Language Modeling for Automatic Speech Recognition},
  author={Feng, Yukun and Tu, Ming and Xia, Rui and Huang, Chuanzeng and Wang, Yuxuan},
  journal={arXiv preprint arXiv:2301.00066},
  year={2022}
}
@article{fevry2020entities,
  title={Entities as experts: Sparse memory access with entity supervision},
  author={F{\'e}vry, Thibault and Soares, Livio Baldini and FitzGerald, Nicholas and Choi, Eunsol and Kwiatkowski, Tom},
  journal={arXiv preprint arXiv:2004.07202},
  year={2020}
}
@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}
@article{khandelwal2019generalization,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:1911.00172},
  year={2019}
}

